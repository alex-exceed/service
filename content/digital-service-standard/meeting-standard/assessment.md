---
layout: text/textblock
---

Your service will be assessed as it moves through the stages of the [service design and delivery process](/service-design-delivery-process/). Depending on the kind of service this can happen as:

* [in-flight assessment](#in-flight-assessment-process) - regular check-ins by a team of experts as you design and develop your service  
* [staged assessment]() - assessments at 3 points in the service design and delivery process.

DTA-led assessments on high volume transactional services will use the model most appropriate for the service and available resources.

For agency self-assessments on all other services, you may choose to use the [in-flight process]() or the [staged assessment process]().

## [2]Assessment changes during the service design and delivery stages

As your service moves through the [service design and delivery process](/service-design-delivery-process/) the assessments become more comprehensive:

1. By the end of **Alpha stage** the service must have passed criteria 1 to 3 of the Standard and show progress against other criteria.
2. During the **Beta stage** the service must have passed all criteria of the Standard before you can release the product as a public beta.
3. At the end of the Beta stage, before you move to the **Live stage** you must have passed all criteria of the Standard.

If your service doesn’t meet the Standard it shouldn’t progress through the service design and delivery process until the relevant criteria of the Standard have been met.

## [2]Assessment team

In-flight and staged assessments are conducted by a team that is independent of the [digital delivery team](/starting-team/roles/). The assessors are experts and practitioners in their field, and will usually be working on their own projects. This ensures that the knowledge and skills of the assessors are up to date.

The assessing team will generally include 3 assessors (one of which is nominated as the lead assessor) who represent:

* service design/user research
* technology
* agile delivery.

You can contact the DTA Standard team by emailing <standard@digital.gov.au> for help on setting up an assessment team.

## [2]In-flight assessment process

The in-flight assessment focuses on continuous improvement and learning for the digital delivery team and the assessor team. It is less about regular tests and more about a mentoring relationship between both teams. Transitioning between the stages should be a point of celebration for both teams.

**Meet with assessors regularly**

Your digital delivery team will meet with the assessment at regular intervals during the Discovery, Alpha and Beta stages, until the service is launched as a public beta.

As the service moves towards the Live stage, the assessment and digital delivery teams may consider moving check-ins to longer intervals, but they should happen at least monthly.

If you are moving from staged assessment to in-flight, you might spend more time on the first check-in meeting to help the assessment team understand the product.

Your meetings should focus on progress against the Digital Service Standard, documented using a red, amber, green (RAG) kanban. You can use the [Digital Service Standard kanban poster (PDF 117 KB)](/assets/files/standard/digital-service-standard-kanban-poster.pdf) to help with this. Both teams work together to make sure the service meets all of the requirements for each criterion in the current stage.

The assessors document what the teams have done well since the last meeting and make recommendations for the next period of work. The assessors will also provide guidance to the team to help them continue to work more closely to the Standard and achieve successful outcomes for their project.

Comments and RAG ratings should be complete by the end of the in-flight session and available for weekly reporting to stakeholders. Assessors will use a spreadsheet to track your progress and record recommendations.

**Agree to transition to the next stage**

Both the assessors and the service team agree when the requirements of the stage have been fulfilled and when the service can progress. This may be indicated by relevant criteria being assessed as green in the RAG rating.

This transition will also be documented by a brief report highlighting the achievements and learnings of the previous stage.

If your service is a high volume transactional service, or you started designing/redesigning it after 6 May 2016, the Alpha, Beta and Live reports will be provided to the DTA for publishing. Services moving to public beta will also start reporting on the [Performance Dashboard](https://www.dta.gov.au/what-we-do/platforms/performance/).